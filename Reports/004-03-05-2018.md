# Week of 02-26-2018

### Done:
  * Studied different methods for comparing distributions of returns. Based on survey and experiments.
    * For `char`: Use cosine similarity
    * For `int` and `float`: Use  [KS test](https://en.wikipedia.org/wiki/Kolmogorovâ€“Smirnov_test)
  * Cluster methods based on these similarity methods.
    * Based on need and use of custom distance metric we choose [DBScan](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html).
    * Can't use classic clustering algorithms like K-means since it uses only euclidian distance. 
    Hierarchical clustering might help and can be used later
  * The clustering method:
    1. Split methods by return type
      * `int`: 885 methods 
      * `float`: 12 methods
      * `char`: 3 methods
    2. Tune the three sub clusters using grid search adjusting `eps` and `min_samples` to minimize outliers and maximize clusters
      * `int`: `eps` = 0.025, `min_samples` = 5 
      * `float`: `eps` = 0.025, `min_samples` = 1
      * `char`: `eps` = 0.2, `min_samples` = 1
    3. Clusters:
      * `int`: clusters: {1: 249, 2: 419, outliers: 197}
      * `float`: clusters: {1: 6, 2: 2, outliers: 4}
      * `char`: clusters: {1: 2, outliers: 1}
      
### Results:
  * Best tuning gives only two clusters which might not be intersting. Possible reasons
    * Too little data
    * Data cannot be grouped.
    * Bad selection of similarity measure
  * Infering quality of clusters?
   
   
### Failed to do:
  * Conclusive evidence if functions can be semantically clustered.
  
  
### Blockers, Challenges, Questions
  * How can quality of clusters be studied.
  * Thoughts on next direction? More data or change approach?
  
### Next week:
  * Subject to answers to blockers and challenges.
